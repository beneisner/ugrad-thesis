\chapter{Conclusion}

If there is one thing that we have learned throughout the experimentation process, it's that making incremental improvements to any stage of the EM Segmentation pipeline is a both a large theoretical and computational challenge. Because there is not a strong theoretical underpinning for the emergent properties of various neural networks, it is often quite difficult to come up with an architecture that learns anything about segmentation, let alone outperforms some existing state-of-the-art model. At the same time, the data processing pipeline becomes more complex as more advanced methods are implemented, and it is inevitable that experimentation velocity will diminish with the number of parameters to tune in the system. It is this duality of problem that inspired this thesis, and now brings us to our conclusion.

If the topics covered in Chapters 1-5 appeared to conceptually follow a logical progression, that was intended. In Chapters 1 and 2, we discussed the practical problems of developing a real-world system for EM Segmentation, and explored various architectural and design solutions to these problems. Our own struggle as researchers with these problems culminated in the development of the presented EM Segmentation framework, \texttt{DeepSeg}. While the \texttt{DeepSeg} framework is far from perfect, it allowed us to build meaningful abstractions as we explored different architectures and pipeline components without worrying too much about how different parts of the pipeline were affected by our explorations. Moreover, it allowed us to easily repeat experiments and keep track of the results we generated.\footnote{In fact, all of the results generated in this paper were generated by repeatable experiments set up in \texttt{Jupyter} notebooks alongside the text of this report.}

Armed with this framework, in Chapter 3 we were able to demonstrate some of the various parameters and dataset attributes that affect the performance and generalizability of models on 2D segmentation. While the problem of 2D segmentation is not particularly exciting, as state-of-the-art models perform on par with human experts, it is a good problem domain in which to concretely explore the effects caused by altering these parameters and dataset attributes. To this end, we showed that deeper networks performed better than shallower ones, and that substantial data augmentation is extremely important to the generalizability of models.

We expanded on the idea of dataset quality in Chapter 4, where we explored the challenge of 3D segmentation on 4 separate datasets. While the original intent of the research project was to build self-contained models that would approach or exceed state-of-the-art methods for 3D EM segmentation, we quickly discovered that not all 3D EM segmentation tasks are not equal. Some datasets are better than others. We did show that there was variance in both the models we trained, and that adding skip connections and residual layers helped enforce some invariants in the look of the output. But the major takeaway from this section is that imaging artifacts, and in particular misalignments, have a major impact on the performance of several classes of models, even if the underlying structures of the data are roughly the same.

In Chapter 5, we attempted - albeit unsuccessfully - to explore automated methods for correcting misalignments in EM stacks using learned methods. We were particularly concerned with Spatial Transformer Networks, which rather than learn an image transformation from scratch simply learn the parameters to a pre-defined image transform, doing so in such a way that error can be propogated up from a downstream task to inform better transforms. While our attempts to apply these techniques to EM stack realignment were not emperically successful, certain results pointed to the notion that while our attempts were incorrect, the approach in general might yield positive results with more experimentation. 

Now, at the conclusion of this thesis, we are now concerned with the implications of our findings. While we did not develop models that achieved state-of-the-art performance in any of the benchmark tasks - we preform admirably, but not at the top of the leaderboards - we developed a set of tools and model paradigms that are good starting points for future research in various areas of the EM segmentation pipeline. In terms of research direction and future work, we are most interested in developing segmentation models that are end-to-end trainable. That is, we are interested in developing an EM segmentation pipeline where each stage is fully (or at least partially) differentiable, allowing direct training of segmentations from raw inputs. While there is still a monumental amount of research requred to create such a pipeline that performs effectively, we believe that replacing the various hand-tuned components of the pipeline with trained components is a promising avenue of research for boosting end-segmentation performance. 