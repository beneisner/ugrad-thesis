Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Lowe1999,
author = {Lowe, D.G.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790410},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Lowe - 1999 - Object recognition from local scale-invariant features.pdf:pdf},
isbn = {0-7695-0164-8},
pages = {1150--1157 vol.2},
publisher = {IEEE},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/document/790410/},
year = {1999}
}
@article{Abadi,
abstract = {TensorFlow [1] is an interface for expressing machine learn-ing algorithms, and an implementation for executing such al-gorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of hetero-geneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learn-ing systems into production across more than a dozen areas of computer science and other fields, including speech recogni-tion, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Abadi et al. - Unknown - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf}
}
@article{Beier2017,
author = {Beier, Thorsten and Pape, Constantin and Rahaman, Nasim and Prange, Timo and Berg, Stuart and Bock, Davi D and Cardona, Albert and Knott, Graham W and Plaza, Stephen M and Scheffer, Louis K and Koethe, Ullrich and Kreshuk, Anna and Hamprecht, Fred A},
doi = {10.1038/nmeth.4151},
issn = {1548-7091},
journal = {Nature Methods},
month = {jan},
number = {2},
pages = {101--102},
publisher = {Nature Research},
title = {{Multicut brings automated neurite segmentation closer to human performance}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.4151},
volume = {14},
year = {2017}
}
@article{Milletari2016,
abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
archivePrefix = {arXiv},
arxivId = {1606.04797},
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
doi = {10.1109/3DV.2016.79},
eprint = {1606.04797},
isbn = {978-1-5090-5407-7},
journal = {arXiv preprint arXiv:1606.04797},
pages = {1--11},
title = {{V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation}},
url = {http://arxiv.org/abs/1606.04797},
year = {2016}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/pdf/1512.03385v1.pdf},
volume = {7},
year = {2015}
}
@article{Cirean,
abstract = {We address a central problem of neuroanatomy, namely, the automatic segmen-tation of neuronal structures depicted in stacks of electron microscopy (EM) im-ages. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succes-sion of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 × 512 × 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
author = {Cirean, Dan C and Giusti, Alessandro and Gambardella, Luca M},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Cirean, Giusti, Gambardella - Unknown - Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images.pdf:pdf},
title = {{Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images}},
url = {http://people.idsia.ch/{~}juergen/nips2012.pdf}
}
@article{Badrinarayanan2015,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps.We compare our proposed architecture with the fully convolutional network [2] architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent without complex training protocols.We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. These quantitative assessments show that SegNet provides competitive performance although it is significantly smaller than other architectures.We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1505.0729},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1103/PhysRevX.5.041024},
eprint = {1505.0729},
issn = {21603308},
journal = {Cvpr 2015},
keywords = {Decoder,Deep Convolutional Neural Networks,Encoder,Pooling,Semantic Pixel-Wise Segmentation,Upsampling},
pages = {5},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {http://arxiv.org/abs/1505.0729{\%}5Cnhttp://mi.eng.cam.ac.uk/projects/segnet/},
year = {2015}
}
@misc{Jaderberg2015,
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and koray Kavukcuoglu},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Jaderberg et al. - 2015 - Spatial Transformer Networks.pdf:pdf},
pages = {2017--2025},
title = {{Spatial Transformer Networks}},
url = {http://papers.nips.cc/paper/5854-spatial-transformer-networks},
year = {2015}
}
@incollection{Mielanczyk2015,
author = {Miela{\'{n}}czyk, {\L}ukasz and Matysiak, Natalia and Klymenko, Olesya and Wojnicz, Romuald},
booktitle = {The Transmission Electron Microscope - Theory and Applications},
doi = {10.5772/60680},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Miela{\'{n}}czyk et al. - 2015 - Transmission Electron Microscopy of Biological Samples.pdf:pdf},
month = {sep},
publisher = {InTech},
title = {{Transmission Electron Microscopy of Biological Samples}},
url = {http://www.intechopen.com/books/the-transmission-electron-microscope-theory-and-applications/transmission-electron-microscopy-of-biological-samples},
year = {2015}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
month = {may},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{Apthorpe2016,
abstract = {Calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously. As calcium imaging datasets grow in size, automated detection of individual neurons is becoming important. Here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed. Accuracy is superior to the popular PCA/ICA method based on precision and recall relative to ground truth annotation by a human expert. These results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data.},
archivePrefix = {arXiv},
arxivId = {1606.07372},
author = {Apthorpe, Noah J. and Riordan, Alexander J. and Aguilar, Rob E. and Homann, Jan and Gu, Yi and Tank, David W. and Seung, H. Sebastian},
eprint = {1606.07372},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Apthorpe et al. - 2016 - Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks.pdf:pdf},
month = {jun},
title = {{Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks}},
url = {http://arxiv.org/abs/1606.07372},
year = {2016}
}
@misc{Arganda-Carreras2013,
author = {Arganda-Carreras, Ignacio and Seung, H. Sebastian and Vishwanathan, Ashwin and Vishwanathan, Daniel R. Berger},
title = {{ISBI 2013 challenge: 3D segmentation of neurites in EM images | SNEMI3D: 3D Segmentation of neurites in EM images}},
url = {http://brainiac2.mit.edu/SNEMI3D/home},
urldate = {2017-03-01},
year = {2013}
}
@article{Lee,
abstract = {Efforts to automate the reconstruction of neural circuits from 3D electron micro-scopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images ac-quired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innova-tions. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Sec-ond, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first net-work generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Back-propagation training is accelerated by ZNN, a new implementation of 3D convo-lutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.},
author = {Lee, Kisuk and Zlateski, Aleksandar and Vishwanathan, Ashwin and Seung, H Sebastian},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - Unknown - Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection.pdf:pdf},
title = {{Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection}},
url = {https://papers.nips.cc/paper/5636-recursive-training-of-2d-3d-convolutional-networks-for-neuronal-boundary-prediction.pdf}
}
@article{Nguyen2015,
abstract = {The ability to acquire large-scale recordings of neuronal activity in awake and unrestrained animals is needed to provide new insights into how populations of neurons generate animal behavior. We present an instrument capable of recording intracellular calcium transients from the majority of neurons in the head of a freely behaving Caenorhabditis elegans with cellular resolution while simultaneously recording the animal's position, posture, and locomotion. This instrument provides whole-brain imaging with cellular resolution in an unrestrained and behaving animal. We use spinning-disk confocal microscopy to capture 3D volumetric fluorescent images of neurons expressing the calcium indicator GCaMP6s at 6 head-volumes/s. A suite of three cameras monitor neuronal fluorescence and the animal's position and orientation. Custom software tracks the 3D position of the animal's head in real time and two feedback loops adjust a motorized stage and objective to keep the animal's head within the field of view as the animal roams freely. We observe calcium transients from up to 77 neurons for over 4 min and correlate this activity with the animal's behavior. We characterize noise in the system due to animal motion and show that, across worms, multiple neurons show significant correlations with modes of behavior corresponding to forward, backward, and turning locomotion.},
archivePrefix = {arXiv},
arxivId = {1501.03463},
author = {Nguyen, Jeffrey P and Shipley, Frederick B and Linder, Ashley N and Plummer, George S and Liu, Mochi and Setru, Sagar U and Shaevitz, Joshua W and Leifer, Andrew M},
doi = {10.1073/pnas.1507110112},
eprint = {1501.03463},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {9},
pages = {33},
pmid = {26712014},
title = {{Whole-brain calcium imaging with cellular resolution in freely behaving Caenorhabditis elegans.}},
url = {http://arxiv.org/abs/1501.03463},
year = {2015}
}
@article{Oh2014,
abstract = {Comprehensive knowledge of the brain's wiring diagram is fundamental for understanding how the nervous system processes information at both local and global scales. However, with the singular exception of the C. elegans microscale connectome, there are no complete connectivity data sets in other species. Here we report a brain-wide, cellular-level, mesoscale connectome for the mouse. The Allen Mouse Brain Connectivity Atlas uses enhanced green fluorescent protein (EGFP)-expressing adeno-associated viral vectors to trace axonal projections from defined regions and cell types, and high-throughput serial two-photon tomography to image the EGFP-labelled axons throughout the brain. This systematic and standardized approach allows spatial registration of individual experiments into a common three dimensional (3D) reference space, resulting in a whole-brain connectivity matrix. A computational model yields insights into connectional strength distribution, symmetry and other network properties. Virtual tractography illustrates 3D topography among interconnected regions. Cortico-thalamic pathway analysis demonstrates segregation and integration of parallel pathways. The Allen Mouse Brain Connectivity Atlas is a freely available, foundational resource for structural and functional investigations into the neural circuits that support behavioural and cognitive processes in health and disease.},
author = {Oh, Seung Wook and Harris, Julie A. and Ng, Lydia and Winslow, Brent and Cain, Nicholas and Mihalas, Stefan and Wang, Quanxin and Lau, Chris and Kuan, Leonard and Henry, Alex M. and Mortrud, Marty T. and Ouellette, Benjamin and Nguyen, Thuc Nghi and Sorensen, Staci A. and Slaughterbeck, Clifford R. and Wakeman, Wayne and Li, Yang and Feng, David and Ho, Anh and Nicholas, Eric and Hirokawa, Karla E. and Bohn, Phillip and Joines, Kevin M. and Peng, Hanchuan and Hawrylycz, Michael J. and Phillips, John W. and Hohmann, John G. and Wohnoutka, Paul and Gerfen, Charles R. and Koch, Christof and Bernard, Amy and Dang, Chinh and Jones, Allan R. and Zeng, Hongkui},
doi = {10.1038/nature13186},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7495},
pages = {207--214},
pmid = {24695228},
title = {{A mesoscale connectome of the mouse brain}},
url = {http://www.nature.com/doifinder/10.1038/nature13186},
volume = {508},
year = {2014}
}
@article{Dosovitskiy2014,
abstract = {Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While such generic features cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.},
archivePrefix = {arXiv},
arxivId = {1406.6909},
author = {Dosovitskiy, Alexey and Fischer, Philipp and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
eprint = {1406.6909},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Dosovitskiy et al. - 2014 - Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks.pdf:pdf},
month = {jun},
title = {{Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1406.6909},
year = {2014}
}
@article{JianboShi2000,
author = {{Jianbo Shi} and Malik, J.},
doi = {10.1109/34.868688},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Jianbo Shi, Malik - 2000 - Normalized cuts and image segmentation.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {888--905},
title = {{Normalized cuts and image segmentation}},
url = {http://ieeexplore.ieee.org/document/868688/},
volume = {22},
year = {2000}
}
@article{Simard2003,
abstract = {Neural Networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple "do-it-yourself" implementation of convolution neural networks does not require complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
author = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
doi = {10.1109/ICDAR.2003.1227801},
isbn = {0-7695-1960-1},
journal = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},
keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural networks,Performance analysis,Support vector machines,Text analysis},
number = {Icdar},
pages = {958--963},
title = {{Best practices for convolutional neural networks applied to visual document analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1227801},
volume = {1},
year = {2003}
}
@article{Turaga2010,
abstract = {Many image segmentation algorithms first generate an affinity graph and then partition it. We present a machine learning approach to computing an affinity graph using a convolutional network (CN) trained using ground truth provided by human experts. The CN affinity graph can be paired with any standard partitioning algorithm and improves segmentation accuracy significantly compared to standard hand-designed affinity functions. We apply our algorithm to the challenging 3D segmentation problem of reconstructing neuronal processes from volumetric electron microscopy (EM) and show that we are able to learn a good affinity graph directly from the raw EM images. Further, we show that our affinity graph improves the segmentation accuracy of both simple and sophisticated graph partitioning algorithms. In contrast to previous work, we do not rely on prior knowledge in the form of hand-designed image features or image preprocessing. Thus, we expect our algorithm to generalize effectively to arbitrary image types.},
author = {Turaga, Srinivas C and Murray, Joseph F and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and Seung, H Sebastian},
doi = {10.1162/neco.2009.10-08-881},
isbn = {1530-888X (Electronic)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
number = {2},
pages = {511--538},
pmid = {19922289},
title = {{Convolutional networks can learn to generate affinity graphs for image segmentation.}},
volume = {22},
year = {2010}
}
@misc{Funke.Jan2016,
author = {{Funke. Jan} and Saalfeld, Stephan and Bock, Davi and Turaga, Srini and Perlman, Eric},
title = {{CREMI: Circuit Reconstruction from Electron Microscopy Images}},
url = {https://cremi.org/},
urldate = {2017-03-01},
year = {2016}
}
@article{Cicek2016,
abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
archivePrefix = {arXiv},
arxivId = {1606.06650},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
eprint = {1606.06650},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/{\c{C}}i{\c{c}}ek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:pdf},
month = {jun},
title = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}},
url = {http://arxiv.org/abs/1606.06650},
year = {2016}
}
@article{Lichtman2014,
author = {Lichtman, Jeff W and Pfister, Hanspeter and Shavit, Nir},
doi = {10.1038/nn.3837},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {oct},
number = {11},
pages = {1448--1454},
publisher = {Nature Research},
title = {{The big data challenges of connectomics}},
url = {http://www.nature.com/doifinder/10.1038/nn.3837},
volume = {17},
year = {2014}
}
@article{Januszewski2016,
abstract = {State-of-the-art image segmentation algorithms generally consist of at least two successive and distinct computations: a boundary detection process that uses local image information to classify image locations as boundaries between objects, followed by a pixel grouping step such as watershed or connected components that clusters pixels into segments. Prior work has varied the complexity and approach employed in these two steps, including the incorporation of multi-layer neural networks to perform boundary prediction, and the use of global optimizations during pixel clustering. We propose a unified and end-to-end trainable machine learning approach, flood-filling networks, in which a recurrent 3d convolutional network directly produces individual segments from a raw image. The proposed approach robustly segments images with an unknown and variable number of objects as well as highly variable object sizes. We demonstrate the approach on a challenging 3d image segmentation task, connectomic reconstruction from volume electron microscopy data, on which flood-filling neural networks substantially improve accuracy over other state-of-the-art methods. The proposed approach can replace complex multi-step segmentation pipelines with a single neural network that is learned end-to-end.},
archivePrefix = {arXiv},
arxivId = {1611.00421},
author = {Januszewski, Micha{\l} and Maitin-Shepard, Jeremy and Li, Peter and Kornfeld, J{\"{o}}rgen and Denk, Winfried and Jain, Viren},
eprint = {1611.00421},
file = {:Users/Ben/Google Drive/Research/References/Januszewski et al/Unknown/Januszewski et al. - 2016 - Flood-Filling Networks.pdf:pdf},
month = {nov},
title = {{Flood-Filling Networks}},
url = {http://arxiv.org/abs/1611.00421},
year = {2016}
}
@article{Haralick1985,
author = {Haralick, Robert M. and Shapiro, Linda G.},
doi = {10.1016/S0734-189X(85)90153-7},
issn = {0734189X},
journal = {Computer Vision, Graphics, and Image Processing},
month = {jan},
number = {1},
pages = {100--132},
title = {{Image segmentation techniques}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0734189X85901537},
volume = {29},
year = {1985}
}
@article{Kasthuri2015,
abstract = {We describe automated technologies to probe the structure of neural tissue at nanometer resolution and use them to generate a saturated reconstruction of a sub-volume of mouse neocortex in which all cellular objects (axons, dendrites, and glia) and many sub-cellular components (synapses, synaptic vesicles, spines, spine apparati, postsynaptic densities, and mitochondria) are rendered and itemized in a database. We explore these data to study physical properties of brain tissue. For example, by tracing the trajectories of all excitatory axons and noting their juxtapositions, both synaptic and non-synaptic, with every dendritic spine we refute the idea that physical proximity is sufficient to predict synaptic connectivity (the so-called Peters' rule). This online minable database provides general access to the intrinsic complexity of the neocortex and enables further data-driven inquiries. Video Abstract},
author = {Kasthuri, Narayanan and Hayworth, Kenneth Jeffrey and Berger, Daniel Raimund and Schalek, Richard Lee and Conchello, Jos?? Angel and Knowles-Barley, Seymour and Lee, Dongil and V??zquez-Reina, Amelio and Kaynig, Verena and Jones, Thouis Raymond and Roberts, Mike and Morgan, Josh Lyskowski and Tapia, Juan Carlos and Seung, H. Sebastian and Roncal, William Gray and Vogelstein, Joshua Tzvi and Burns, Randal and Sussman, Daniel Lewis and Priebe, Carey Eldin and Pfister, Hanspeter and Lichtman, Jeff William},
doi = {10.1016/j.cell.2015.06.054},
isbn = {1097-4172 (Electronic)$\backslash$r0092-8674 (Linking)},
issn = {10974172},
journal = {Cell},
number = {3},
pages = {648--661},
pmid = {26232230},
title = {{Saturated Reconstruction of a Volume of Neocortex}},
volume = {162},
year = {2015}
}
@article{Arganda-Carreras2015,
abstract = {To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic (EM) images of the brain. Participants submitted boundary maps predicted for a test set of images, and were scored based on their agreement with ground truth from human experts. The winning team had no prior experience with EM images, and employed a convolutional network. This ``deep learning'' approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from cooperation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring. Retrospective evaluation of the challenge scoring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge.},
author = {Arganda-Carreras, Ignacio and Turaga, Srinivas C. and Berger, Daniel R. and Cireşan, Dan and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen and Laptev, Dmitry and Dwivedi, Sarvesh and Buhmann, Joachim M. and Liu, Ting and Seyedhosseini, Mojtaba and Tasdizen, Tolga and Kamentsky, Lee and Burget, Radim and Uher, Vaclav and Tan, Xiao and Sun, Changming and Pham, Tuan D. and Bas, Erhan and Uzunbas, Mustafa G. and Cardona, Albert and Schindelin, Johannes and Seung, H. Sebastian},
doi = {10.3389/fnana.2015.00142},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Arganda-Carreras et al. - 2015 - Crowdsourcing the creation of image segmentation algorithms for connectomics.pdf:pdf},
issn = {1662-5129},
journal = {Frontiers in Neuroanatomy},
keywords = {Electron microscopy,connectomics,image segmentation,machine learning,reconstruction},
month = {nov},
pages = {142},
publisher = {Frontiers},
title = {{Crowdsourcing the creation of image segmentation algorithms for connectomics}},
url = {http://journal.frontiersin.org/Article/10.3389/fnana.2015.00142/abstract},
volume = {9},
year = {2015}
}
@article{White1986,
abstract = {The structure and connectivity of the nervous system of the nematode Caenorhabditis elegans has been deduced from reconstructions of electron micrographs of serial sections. The hermaphrodite nervous system has a total complement of 302 neurons, which are arranged in an essentially invariant structure. Neurons with similar morphologies and connectivities have been grouped together into classes; there are 118 such classes. Neurons have simple morphologies with few, if any, branches. Processes from neurons run in defined positions within bundles of parallel processes, synaptic connections being made en passant. Process bundles are arranged longitudinally and circumferentially and are often adjacent to ridges of hypodermis. Neurons are generally highly locally connected, making synaptic connections with many of their neighbours. Muscle cells have arms that run out to process bundles containing motoneuron axons. Here they receive their synaptic input in defined regions along the surface of the bundles, where motoneuron axons reside. Most of the morphologically identifiable synaptic connections in a typical animal are described. These consist of about 5000 chemical synapses, 2000 neuromuscular junctions and 600 gap junctions.},
author = {White, J G and Southgate, E. and Thomson, J N and Brenner, S.},
doi = {10.1098/rstb.1986.0056},
isbn = {0962-8436 (Print)$\backslash$r0962-8436 (Linking)},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society of London},
number = {1165},
pages = {1--340},
pmid = {22462104},
title = {{The structure of the nervous system of the nematode Caenorhabditis elegans.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22462104},
volume = {314},
year = {1986}
}
@article{Long2014,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
eprint = {1411.4038},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2014 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
month = {nov},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1411.4038},
year = {2014}
}
@article{Lecun1998,
author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
file = {:Users/Ben/Library/Application Support/Mendeley Desktop/Downloaded/Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
